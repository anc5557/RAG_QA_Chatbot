import base64
import os
import tempfile
import time
import streamlit as st
import uuid
import json
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.document_loaders import PyMuPDFLoader
from langchain_community.vectorstores import FAISS
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain.chains import create_history_aware_retriever, create_retrieval_chain
from langchain_community.llms import Ollama
from langchain_core.prompts import (
    ChatPromptTemplate,
    MessagesPlaceholder,
)
from langchain.chains.combine_documents import create_stuff_documents_chain
from langchain_core.documents import Document


def initialize_session():
    """
    ì„¸ì…˜ì„ ì´ˆê¸°í™”í•˜ê³  í•„ìš”í•œ ì„¸ì…˜ ìƒíƒœ ë³€ìˆ˜ë¥¼ ì„¤ì •í•©ë‹ˆë‹¤.

    ì´ í•¨ìˆ˜ëŠ” ì„¸ì…˜ ìƒíƒœì— "id" í‚¤ê°€ ìˆëŠ”ì§€ í™•ì¸í•©ë‹ˆë‹¤. ì—†ìœ¼ë©´ ìƒˆë¡œìš´ UUIDë¥¼ ìƒì„±í•˜ê³  "id" í‚¤ì— í• ë‹¹í•©ë‹ˆë‹¤.
    ë˜í•œ ì„¸ì…˜ ìƒíƒœì˜ "file_cache", "messages", "rag_chain" í‚¤ë¥¼ ì´ˆê¸°í™”í•©ë‹ˆë‹¤.
    """
    if "id" not in st.session_state:
        st.session_state.id = uuid.uuid4()  # ì„¸ì…˜ ID
        st.session_state.file_cache = {}  # íŒŒì¼ ìºì‹œë¥¼ ì €ì¥í•˜ëŠ” ë”•ì…”ë„ˆë¦¬
        st.session_state.messages = []  # ì±—ë´‡ ëŒ€í™”ë¥¼ ì €ì¥í•˜ëŠ” ë¦¬ìŠ¤íŠ¸
        st.session_state.rag_chain = None  # RAG ì²´ì¸ì„ ì €ì¥í•˜ëŠ” ë³€ìˆ˜


def reset_chat():
    """
    ì±—ë´‡ ëŒ€í™”ë¥¼ ì´ˆê¸°í™”í•©ë‹ˆë‹¤.
    """
    st.session_state.messages = []
    st.session_state.context = None


def display_pdf(file):
    """
    PDF íŒŒì¼ì„ ë¯¸ë¦¬ë³´ê¸°í•©ë‹ˆë‹¤.
    - í˜„ì¬ pdfê°€ ì„¸ë¡œì¸ ê²½ìš° ë³´ì´ì§€ ì•ŠëŠ” ë¬¸ì œê°€ ìˆìŒ

    Args: file (BytesIO): PDF íŒŒì¼
    """
    st.markdown("### PDF Preview")
    file.seek(0)
    base64_pdf = base64.b64encode(file.read()).decode("utf-8")
    pdf_display = f"""<iframe src="data:application/pdf;base64,{base64_pdf}" width="100%" height="600px" type="application/pdf" style="border: none;"></iframe>"""
    st.markdown(pdf_display, unsafe_allow_html=True)


def combine_title_description(data):
    """JSON ë°ì´í„°ì—ì„œ titleê³¼ descriptionì„ í•©ì¹©ë‹ˆë‹¤.

    ì´ ë¶€ë¶„ì€ ì‚¬ìš©ìì˜ ë°ì´í„° êµ¬ì¡°ì— ë”°ë¼ ìˆ˜ì •ì´ í•„ìš”í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

    Args:
        data (list): JSON ë°ì´í„°

    Returns:
        list: title, description í•©ì¹œ ë°ì´í„°
    """
    combined_docs = []
    for manual in data:
        for item in manual["manual"]:
            combined_docs.append(
                {"title": item["title"], "description": item["description"]}
            )
    return combined_docs


def process_pdf(file, file_key):
    """PDFë¥¼ ì¸ë±ì‹±í•˜ê³  RAG ì²´ì¸ì„ ìƒì„±í•©ë‹ˆë‹¤.

    Args:
        file (BytesIO): PDF íŒŒì¼
        file_key (str): íŒŒì¼ í‚¤
    """
    with tempfile.TemporaryDirectory() as temp_dir:
        file_path = os.path.join(temp_dir, file.name)
        with open(file_path, "wb") as f:
            f.write(file.getvalue())

        if file_key not in st.session_state.file_cache:
            loader = PyMuPDFLoader(file_path)
            embedding_model = HuggingFaceEmbeddings(
                model_name="jhgan/ko-sroberta-multitask"
            )

            pages = loader.load()
            text_splitter = RecursiveCharacterTextSplitter(
                chunk_size=300, chunk_overlap=0
            )
            split_pages = text_splitter.split_documents(pages)

            vectorstore = FAISS.from_documents(
                documents=split_pages, embedding=embedding_model
            )

            retriever = vectorstore.as_retriever(
                search_type="similarity", search_kwargs={"k": 2}
            )

            # llama-3-Korean-Bllossom-8B-gguf-Q4_K_M or EEVE-Korean-10.8B-Q5_K_M-GGUF
            llm = Ollama(model="EEVE-Korean-10.8B-Q5_K_M-GGUF")

            # ì§ˆë¬¸ ì¬êµ¬ì„± ì²´ì¸ ìƒì„±
            question_rephrasing_chain = create_question_rephrasing_chain(llm, retriever)
            # ì§ˆë¬¸ ë‹µë³€ ì²´ì¸ ìƒì„±
            question_answering_chain = create_question_answering_chain(llm)

            # RAG ì²´ì¸ ì—°ê²°(ì§ˆë¬¸ ì¬êµ¬ì„± -> ì§ˆë¬¸ ë‹µë³€)
            rag_chain = create_retrieval_chain(
                question_rephrasing_chain, question_answering_chain
            )

            st.session_state.file_cache[file_key] = rag_chain

        st.session_state.rag_chain = st.session_state.file_cache[file_key]


def process_json(file, file_key):
    """JSON ë°ì´í„°ë¥¼ ì¸ë±ì‹±í•˜ê³  RAG ì²´ì¸ì„ ìƒì„±í•©ë‹ˆë‹¤.

    ì¸ë±ì‹± ëŒ€ìƒ ë°ì´í„°ëŠ” titleê³¼ descriptionìœ¼ë¡œ êµ¬ì„±ëœ ë°ì´í„°ì—¬ì•¼ í•©ë‹ˆë‹¤.

    Args:
        file (BytesIO): JSON íŒŒì¼
        file_key (str): íŒŒì¼ í‚¤
    """
    data = json.load(file)
    documents = combine_title_description(data)

    embedding_model = HuggingFaceEmbeddings(model_name="jhgan/ko-sroberta-multitask")
    texts = [
        doc["title"] + " " + doc["description"] for doc in documents
    ]  # titleê³¼ descriptionì„ í•©ì¹¨
    document_objects = [
        Document(page_content=text) for text in texts
    ]  # Document ê°ì²´ ìƒì„±

    vectorstore = FAISS.from_documents(
        documents=document_objects, embedding=embedding_model
    )

    retriever = vectorstore.as_retriever(
        search_type="mmr", search_kwargs={"k": 1, "fetch_k": 5}
    )
    llm = Ollama(
        model="EEVE-Korean-10.8B-Q5_K_M-GGUF"
    )  # llama-3-Korean-Bllossom-8B-gguf-Q4_K_M or EEVE-Korean-10.8B-Q5_K_M-GGUF

    question_rephrasing_chain = create_question_rephrasing_chain(llm, retriever)
    question_answering_chain = create_question_answering_chain(llm)

    rag_chain = create_retrieval_chain(
        question_rephrasing_chain, question_answering_chain
    )
    st.session_state.file_cache[file_key] = rag_chain
    st.session_state.rag_chain = st.session_state.file_cache[file_key]


def create_question_rephrasing_chain(llm, retriever):
    """ì§ˆë¬¸ ì¬êµ¬ì„± ì²´ì¸ì„ ìƒì„±í•©ë‹ˆë‹¤.

    Args:
        llm (Ollama): Ollama ê°ì²´
        retriever (Retriever): Retriever ê°ì²´

    Returns:
        Chain: ì§ˆë¬¸ ì¬êµ¬ì„± ì²´ì¸
    """

    system_prompt = """
    ë‹¹ì‹ ì€ ì§ˆë¬¸ ì¬êµ¬ì„±ìì…ë‹ˆë‹¤. ì´ì „ ëŒ€í™” ë‚´ìš©ê³¼ ìµœì‹  ì‚¬ìš©ì ì§ˆë¬¸ì´ ìˆì„ ë•Œ, ì´ ì§ˆë¬¸ì´ ì´ì „ ëŒ€í™” ë‚´ìš©ê³¼ ê´€ë ¨ì´ ìˆì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
    ì´ëŸ° ê²½ìš°, ëŒ€í™” ë‚´ìš©ì„ ì•Œ í•„ìš” ì—†ì´ ë…ë¦½ì ìœ¼ë¡œ ì´í•´í•  ìˆ˜ ìˆëŠ” ì§ˆë¬¸ìœ¼ë¡œ ë°”ê¾¸ì„¸ìš”. 
    ì´ ì¬êµ¬ì„±ëœ ì§ˆë¬¸ì€ ë¬¸ì„œ ê²€ìƒ‰ì—ë§Œ ì‚¬ìš©ë©ë‹ˆë‹¤. ì‚¬ìš©ìì—ê²Œ ì œê³µí•  ìµœì¢… ë‹µë³€ì—ëŠ” ì˜í–¥ì„ ë¯¸ì¹˜ì§€ ì•ŠìŠµë‹ˆë‹¤.
    ê´€ë ¨ì´ ì—†ëŠ” ê²½ìš°, ì§ˆë¬¸ì„ ê·¸ëŒ€ë¡œ ë‘ì„¸ìš”. ì ˆëŒ€ ì§ˆë¬¸ì— ë‹µë³€ì„ ì œê³µí•˜ì§€ ë§ˆì„¸ìš”.
    
    ì˜ˆì‹œ:
    ê´€ë ¨ ìˆëŠ” ê²½ìš°)
    Human: ë©”ì¼ì„ ë°±ì—…í•˜ê³  ì‹¶ì–´
    AI: ë©”ì¼ ë°±ì—…ì€ ê¸°ë³¸ë©”ì¼í•¨ ê´€ë¦¬ > ë‚´ ë©”ì¼í•¨ ê´€ë¦¬ì—ì„œ ê°€ëŠ¥í•©ë‹ˆë‹¤. ë‹¤ìš´ë¡œë“œ ë²„íŠ¼ì„ ì´ìš©í•´ ë©”ì¼í•¨ì„ zip íŒŒì¼ë¡œ ë‹¤ìš´ë¡œë“œí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì›í•˜ëŠ” ê¸°ê°„ì˜ ë©”ì¼ì„ ë°±ì—…í•˜ë ¤ë©´, ê¸°ê°„ë³„ ë°±ì—…ì„ ì²´í¬í•˜ì„¸ìš”. ë°±ì—…í•œ ë©”ì¼ì€ ë‹¤ìš´ë¡œë“œí•œ íŒŒì¼ì„ ì—…ë¡œë“œí•˜ì—¬ ë‹¤ì‹œ ê°€ì ¸ì˜¬ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
    Human: ì—…ë¡œë“œëŠ” ì–´ë–»ê²Œ í•˜ë‚˜ìš”?
    ë‹µë³€: ë°±ì—…í•œ ë©”ì¼ì„ ì—…ë¡œë“œí•˜ëŠ” ë°©ë²•ì€ ë¬´ì—‡ì¸ê°€ìš”?
    
    ê´€ë ¨ ìˆëŠ” ê²½ìš°)
    Human: ë©”ì¼ ì²¨ë¶€íŒŒì¼ í¬ê¸° ì œí•œì´ ìˆë‚˜ìš”?
    AI: ì¼ë°˜ ì²¨ë¶€íŒŒì¼ì˜ ê²½ìš° 20MB, ëŒ€ìš©ëŸ‰ íŒŒì¼ ì²¨ë¶€ì˜ ê²½ìš° 2GBê¹Œì§€ ê°€ëŠ¥í•©ë‹ˆë‹¤.
    Human: í˜•ì‹ì— ì œí•œì´ ìˆë‚˜ìš”?
    ë‹µë³€: ë©”ì¼ ì²¨ë¶€íŒŒì¼ í˜•ì‹ ì œí•œì´ ìˆë‚˜ìš”?
    
    ê´€ë ¨ ì—†ëŠ” ê²½ìš°)
    Human: ë©”ì¼ ì²¨ë¶€íŒŒì¼ í¬ê¸° ì œí•œì´ ìˆë‚˜ìš”?
    AI: ì¼ë°˜ ì²¨ë¶€íŒŒì¼ì˜ ê²½ìš° 20MB, ëŒ€ìš©ëŸ‰ íŒŒì¼ ì²¨ë¶€ì˜ ê²½ìš° 2GBê¹Œì§€ ê°€ëŠ¥í•©ë‹ˆë‹¤.
    Human: ì£¼ì†Œë¡ì— ì£¼ì†Œë¥¼ ì´ë™/ë³µì‚¬í•˜ë ¤ë©´ ì–´ë–»ê²Œ í•˜ë‚˜ìš”?
    ë‹µë³€: ì£¼ì†Œë¡ì— ì£¼ì†Œë¥¼ ì´ë™/ë³µì‚¬í•˜ë ¤ë©´ ì–´ë–»ê²Œ í•˜ë‚˜ìš”?
    
    ê´€ë ¨ ì—†ëŠ” ê²½ìš°)
    Human: ì¼ì • ë“±ë¡í•˜ëŠ” ë°©ë²•ì„ ì•Œë ¤ì¤˜
    AI: ì¼ì • ë“±ë¡ ë²„íŠ¼ì„ ëˆ„ë¥´ê±°ë‚˜ ë‚ ì§œë¥¼ ì„ íƒí•´ ë“±ë¡í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì œëª©ê³¼ ì¼ì‹œë¥¼ ì •í•œ í›„, ìº˜ë¦°ë”ì˜ ì¢…ë¥˜ë¥¼ ì„ íƒí•˜ê³ , ì•ŒëŒì„ í†µí•´ ë¯¸ë¦¬ ì¼ì •ì„ ì•Œë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
    Human: ë©”ì¼ ì²¨ë¶€íŒŒì¼ í¬ê¸° ì œí•œì€?
    ë‹µë³€: ë©”ì¼ ì²¨ë¶€íŒŒì¼ í¬ê¸° ì œí•œì€?
    """

    prompt = ChatPromptTemplate.from_messages(
        [
            ("system", system_prompt),
            MessagesPlaceholder(variable_name="chat_history"),
            ("human", "{input}"),
        ]
    )

    # create_history_aware_retriever : ëŒ€í™” ë‚´ì—­ì„ ê°€ì ¸ì™€ ë¬¸ì„œë¥¼ ë°˜í™˜í•˜ëŠ” ì²´ì¸
    # - chat_historyê°€ ì—†ìœ¼ë©´ ì…ë ¥ ì€ ê²€ìƒ‰ê¸°ë¡œ ì§ì ‘ ì „ë‹¬
    # - chat_historyê°€ ìˆìœ¼ë©´ í”„ë¡¬í”„íŠ¸ì™€ LLMì´ ê²€ìƒ‰ ì¿¼ë¦¬ë¥¼ ìƒì„±í•˜ëŠ” ë° ì‚¬ìš©
    return create_history_aware_retriever(llm, retriever, prompt)


def create_question_answering_chain(llm):
    """ì§ˆë¬¸ ë‹µë³€ ì²´ì¸ì„ ìƒì„±í•©ë‹ˆë‹¤.
    ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸: ì—­í• , ê·œì¹™ ê·¸ë¦¬ê³  ì˜ˆì‹œë¥¼ ì„¤ëª…í•˜ê³  ë¬¸ì„œë¥¼ ì œê³µí•©ë‹ˆë‹¤.

    Args:
        llm (Ollama): Ollama ê°ì²´

    Returns:
        Chain: ì§ˆë¬¸ ë‹µë³€ ì²´ì¸
    """
    system_prompt = """ë‹¹ì‹ ì€ í¬ë¦¬ë‹ˆí‹° Q&A ì±—ë´‡ì…ë‹ˆë‹¤. ê²€ìƒ‰ëœ ë¬¸ì„œë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ë‹µë³€í•˜ì„¸ìš”.
    
    ì˜ˆì‹œ:
    ğŸ“ì‚¬ìš©ì ì§ˆë¬¸: í•œë²ˆì— ì—…ë¡œë“œ ê°€ëŠ¥í•œ íŒŒì¼ ê°¯ìˆ˜ëŠ” ëª‡ê°œì¸ê°€ìš”?
    ğŸ“ë‹µë³€: í•œë²ˆì— ì—…ë¡œë“œ ê°€ëŠ¥í•œ ê°¯ìˆ˜ê°€ ì •í•´ì ¸ìˆì§€ ì•Šì§€ë§Œ, ì¼ë°˜ ì²¨ë¶€ê°™ì€ê²½ìš° 20MB ,ëŒ€ìš©ëŸ‰ íŒŒì¼ ì²¨ë¶€ì˜ ê²½ìš° 2048MBê¹Œì§€ ê°€ëŠ¥í•©ë‹ˆë‹¤.
    
    ğŸ“ì‚¬ìš©ì ì§ˆë¬¸: í•´ì™¸ì—ì„œ ë©”ì¼ ì‚¬ìš©ì´ ê°€ëŠ¥í•œê°€ìš”?
    ğŸ“ë‹µë³€: í™˜ê²½ì„¤ì • - ê°œì¸ì •ë³´/ë³´ì•ˆ ê¸°ëŠ¥ - ë³´ì•ˆ ì„¤ì •ì—ì„œ êµ­ê°€ë³„ ë¡œê·¸ì¸ í—ˆìš© ê¸°ëŠ¥ì„ ì´ìš©í•˜ì‹œë©´ ë©ë‹ˆë‹¤. ë³´ì•ˆ ì„¤ì •íƒ­ì´ ë³´ì´ì§€ ì•Šì„ ì‹œì— ë©”ì¼ ë‹´ë‹¹ìì—ê²Œ ë¬¸ì˜í•´ì£¼ì„¸ìš”. 
    
    ğŸ“ì‚¬ìš©ì ì§ˆë¬¸: ì—¬ëŸ¬ëª…ì—ê²Œ ê°œë³„ ë°œì†¡í•˜ê³  ì‹¶ì–´ìš”
    ğŸ“ë‹µë³€: ë©”ì¼ ê°œë³„ë°œì†¡ ì„¤ì •ì— ëŒ€í•´ ì•ˆë‚´ë“œë¦¬ê² ìŠµë‹ˆë‹¤. ê°œë³„ë°œì†¡ì´ë€ ì—¬ëŸ¬ ì‚¬ëŒì—ê²Œ ë™ì‹œì— ë©”ì¼ì„ ë³´ë‚´ë„ ë°›ëŠ”ì‚¬ëŒ ì˜ì—­ì— ìˆ˜ì‹ ì¸ ë³¸ì¸ í•œ ëª…ë§Œ í‘œì‹œë˜ëŠ” ê¸°ëŠ¥ì…ë‹ˆë‹¤. ê¸°ë³¸ì ìœ¼ë¡œëŠ” ì„¤ì •ë˜ì–´ìˆì§€ ì•Šì§€ë§Œ, ë©”ì¼ì“°ê¸° íƒ­ì˜ ë³´ë‚´ê¸° ì„¤ì •ì—ì„œ í•œëª…ì”© ë°œì†¡ì„ ì²´í¬í•˜ì‹œë©´ ì‚¬ìš©í•˜ì‹¤ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

    
    ## ê²€ìƒ‰ëœ ë¬¸ì„œì…ë‹ˆë‹¤. ê° ë¬¸ì„œëŠ” ë¹ˆì¤„ë¡œ êµ¬ë¶„ë˜ì–´ ìˆìŠµë‹ˆë‹¤.
    {context}
    
    ë¬¸ì„œì— ì—†ëŠ” ì •ë³´ëŠ” ë§Œë“¤ì–´ë‚´ì§€ ë§ˆì„¸ìš”. í•œêµ­ì–´ë¡œ ë‹µë³€í•´ì£¼ì„¸ìš”. ì„¸ ë¬¸ì¥ ì´ë‚´ë¡œ ë‹µë³€í•´ì£¼ì„¸ìš”. ëª¨ë¥¸ë‹¤ë©´, ëª¨ë¥¸ë‹¤ê³  ë§í•´ì£¼ì„¸ìš”. ì˜ˆì‹œì™€ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ë¥¼ ë‹µë³€ì— í¬í•¨í•˜ë©´ ì•ˆë©ë‹ˆë‹¤.
    """

    prompt = ChatPromptTemplate.from_messages(
        [
            ("system", system_prompt),
            MessagesPlaceholder(variable_name="chat_history"),
            ("human", "{input}\n"),
        ]
    )

    return create_stuff_documents_chain(llm, prompt)


def clean_data(data, is_pdf=True):
    """í™”ë©´ì— í‘œì‹œí•  ë°ì´í„°ë¥¼ ì •ì œí•©ë‹ˆë‹¤.

    Args:
        data (list): ì°¸ê³ í•œ ë¬¸ì„œ ë°ì´í„°
        is_pdf (bool, optional): PDF íŒŒì¼ ì—¬ë¶€.(PDFì¸ ê²½ìš° í˜ì´ì§€ ì •ë³´ê°€ ìˆê¸° ë•Œë¬¸ì— JSONê³¼ êµ¬ë¶„)

    Returns:
        cleaned_data (list): ì •ì œëœ ë°ì´í„°

    """
    cleaned_data = []
    for item in data:
        cleaned_page_content = item.page_content.replace("\n", " ").strip()
        cleaned_metadata = {"page": item.metadata["page"]} if is_pdf else {}
        cleaned_data.append(
            {"page_content": cleaned_page_content, "metadata": cleaned_metadata}
        )
    return cleaned_data


def main():
    """ë©”ì¸ í•¨ìˆ˜(streamlit ì•±)
    1. ì„¸ì…˜ ì´ˆê¸°í™”
    2. ì‚¬ì´ë“œë°”
        pdf ë˜ëŠ” json íŒŒì¼ ì—…ë¡œë“œ
    3. ì±—ë´‡
        - ëŒ€í™” ì´ˆê¸°í™” ë²„íŠ¼
        - ì‚¬ìš©ì ì…ë ¥
        - ì±—ë´‡ ì‘ë‹µ
        - ì°¸ê³  ë¬¸ì„œ ì •ë³´
    """
    initialize_session()
    session_id = st.session_state.id

    with st.sidebar:
        st.header("ë¬¸ì„œ ì—…ë¡œë“œ")
        uploaded_file = st.file_uploader(
            "PDF ë˜ëŠ” JSON í˜•ì‹ì˜ íŒŒì¼ì„ ì—…ë¡œë“œ í•´ì£¼ì„¸ìš”.", type=["pdf", "json"]
        )

        if uploaded_file:
            try:
                file_key = f"{session_id}-{uploaded_file.name}"
                if uploaded_file.type == "application/pdf":
                    st.write("PDF ë¬¸ì„œë¥¼ ì¸ë±ì‹± í•˜ê³  ìˆìŠµë‹ˆë‹¤...")
                    process_pdf(uploaded_file, file_key)
                    st.write("ì„±ê³µì ìœ¼ë¡œ PDF ë¬¸ì„œë¥¼ ì¸ë±ì‹±í–ˆìŠµë‹ˆë‹¤!")
                    display_pdf(uploaded_file)
                elif uploaded_file.type == "application/json":
                    st.write("JSON ë¬¸ì„œë¥¼ ì¸ë±ì‹± í•˜ê³  ìˆìŠµë‹ˆë‹¤...")
                    process_json(uploaded_file, file_key)
                    st.write("ì„±ê³µì ìœ¼ë¡œ JSON ë¬¸ì„œë¥¼ ì¸ë±ì‹±í–ˆìŠµë‹ˆë‹¤!")
                st.write("ì±—ë´‡ì„ ì‚¬ìš©í•  ì¤€ë¹„ê°€ ë˜ì—ˆìŠµë‹ˆë‹¤!")
            except Exception as e:
                st.write(f"Error: {e}")
                st.stop()

    st.title("RAG Chatbot")

    if st.button("ëŒ€í™” ì´ˆê¸°í™”"):
        reset_chat()
        st.toast("ì´ˆê¸°í™” ë˜ì—ˆìŠµë‹ˆë‹¤.", icon="âŒ")

    # streamlitì—ì„œ ì§€ì›í•˜ëŠ” ì±—ë´‡ UI ì‚¬ìš©
    for message in st.session_state.messages:
        with st.chat_message(message["role"]):
            st.markdown(message["content"])

    if st.session_state.rag_chain is not None:
        if prompt := st.chat_input("Ask a question!"):
            MAX_MESSAGES_BEFORE_DELETION = 4  # ì±—ë´‡ ëŒ€í™” ìµœëŒ€ ì €ì¥ ê°¯ìˆ˜

            # ì±—ë´‡ ëŒ€í™” ìµœëŒ€ ì €ì¥ ê°¯ìˆ˜ë¥¼ ë„˜ì–´ê°€ë©´ ê°€ì¥ ì˜¤ë˜ëœ ëŒ€í™”ë¥¼ ì‚­ì œ
            if len(st.session_state.messages) >= MAX_MESSAGES_BEFORE_DELETION:
                del st.session_state.messages[0]
                del st.session_state.messages[0]

            with st.chat_message("user"):
                st.markdown(prompt)

            with st.chat_message("assistant"):
                message_placeholder = st.empty()
                full_response = ""

                rag_chain = st.session_state.rag_chain
                result = rag_chain.invoke(
                    {"input": prompt, "chat_history": st.session_state.messages}
                )  # ì±—ë´‡ ì²´ì¸ ì‹¤í–‰

                st.session_state.messages.append({"role": "user", "content": prompt})

                cleaned_datas = clean_data(
                    result["context"], is_pdf=uploaded_file.type == "application/pdf"
                )

                for cleaned_data in cleaned_datas:
                    with st.expander("Evidence context"):
                        st.write(f"Page content: {cleaned_data['page_content']}")
                        if (
                            "metadata" in cleaned_data
                            and "page" in cleaned_data["metadata"]
                        ):
                            st.write(f"Page: {cleaned_data['metadata']['page']+1}")

                # ì±—ë´‡ ì‘ë‹µì„ ì¡°ê°ë‚´ì–´ ë³´ì—¬ì¤Œ
                for chunk in result["answer"].split(" "):
                    full_response += chunk + " "
                    time.sleep(0.2)
                    message_placeholder.markdown(full_response + "â–Œ")
                    message_placeholder.markdown(full_response)

            st.session_state.messages.append(
                {"role": "assistant", "content": full_response}
            )
    else:
        st.write("ì‚¬ì´ë“œë°”ì—ì„œ PDF ë˜ëŠ” JSON íŒŒì¼ì„ ì—…ë¡œë“œ í•´ì£¼ì„¸ìš”.")


main()
